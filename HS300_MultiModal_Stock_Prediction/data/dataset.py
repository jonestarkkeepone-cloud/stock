
# ============================================================================
# data/dataset.py
# ============================================================================
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from torch.utils.data import Dataset
from PIL import Image
import torchvision.transforms as transforms

from utils.data_utils import load_time_series, load_news_sentiment, load_fundamentals
from utils.preprocessing import MultimodalPreprocessor

class FinMultiTimeDataset(Dataset):
    """å¤šæ¨¡æ€è‚¡ç¥¨æ•°æ®é›†ï¼ˆä½¿ç”¨æ··åˆCNNæå–Kçº¿ç‰¹å¾ï¼‰"""
    
    def __init__(self, data_dir, market, stocks, start_date, end_date,
                 seq_length, pred_horizon,
                 use_cnn_features=True, cnn_feature_dim=64,
                 preprocessor=None):

        self.base_data_dir = Path(data_dir)
        self.market = market

        # é€‚é…Finmultimeç›®å½•ç»“æ„
        # å¦‚æœæ˜¯Finmultimeç›®å½•ï¼Œä½¿ç”¨ç‰¹æ®Šçš„è·¯å¾„ç»“æ„
        if 'Finmultime' in str(data_dir):
            # Finmultimeçš„ç›®å½•ç»“æ„ä¸åŒï¼Œä¸éœ€è¦æ·»åŠ marketå­ç›®å½•
            self.data_dir = self.base_data_dir
            self.is_finmultime = True
        else:
            # FinMultiTimeçš„æ ‡å‡†ç»“æ„: data_dir/market/
            self.data_dir = self.base_data_dir / market
            self.is_finmultime = False

        self.seq_length = seq_length
        self.pred_horizon = pred_horizon
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.use_cnn_features = use_cnn_features
        
        if stocks is None:
            # å¦‚æœæ˜¯Finmultimeçš„HS300ï¼Œå°è¯•åŠ è½½å®Œæ•´å››æ¨¡æ€è‚¡ç¥¨åˆ—è¡¨
            if self.is_finmultime and self.market == 'HS300':
                stocks = self._load_complete_hs300_stocks()
                if not stocks:
                    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°è‡ªåŠ¨å‘ç°
                    stocks = self._discover_stocks()
            else:
                stocks = self._discover_stocks()
        self.stocks = stocks
        
        self.preprocessor = preprocessor or MultimodalPreprocessor()
        
        # åˆå§‹åŒ–CNNç¼–ç å™¨
        if use_cnn_features:
            from models.image_encoder import HybridKLineEncoder
            self.image_encoder = HybridKLineEncoder(feature_dim=cnn_feature_dim)
            self.image_encoder.eval()
            
            self.image_transform = transforms.Compose([
                transforms.Grayscale(),
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5], std=[0.5])
            ])
        
        self.samples = []
        self._load_and_align_data()
    
    def _load_complete_hs300_stocks(self):
        """åŠ è½½HS300å®Œæ•´å››æ¨¡æ€è‚¡ç¥¨åˆ—è¡¨"""
        import json
        stock_file = Path('hs300_complete_stocks.json')

        if not stock_file.exists():
            print(f"âš ï¸  æœªæ‰¾åˆ° {stock_file}ï¼Œå°†è‡ªåŠ¨å‘ç°è‚¡ç¥¨")
            return None

        try:
            with open(stock_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            stock_codes = data['stock_codes']
            print(f"âœ… åŠ è½½äº† {len(stock_codes)} ä¸ªå®Œæ•´å››æ¨¡æ€HS300è‚¡ç¥¨")
            return stock_codes
        except Exception as e:
            print(f"âš ï¸  åŠ è½½è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def _discover_stocks(self):
        """è‡ªåŠ¨å‘ç°å¯ç”¨çš„è‚¡ç¥¨ä»£ç """
        if self.is_finmultime:
            # Finmultimeç»“æ„: time_series/S%26P500_time_series/S&P500_time_series/ æˆ– time_series/HS300_time_series/HS300_time_series/
            if self.market == 'SP500':
                # æ³¨æ„ï¼šå¤–å±‚ç›®å½•æ˜¯ S%26P500_time_series (URLç¼–ç )ï¼Œå†…å±‚æ˜¯ S&P500_time_series
                ts_dir = self.data_dir / 'time_series' / 'S%26P500_time_series' / 'S&P500_time_series'
            else:  # HS300
                ts_dir = self.data_dir / 'time_series' / 'HS300_time_series' / 'HS300_time_series'
        else:
            # FinMultiTimeæ ‡å‡†ç»“æ„: market/time_series/
            ts_dir = self.data_dir / 'time_series'

        if not ts_dir.exists():
            raise ValueError(f"Directory not found: {ts_dir}")

        # æå–è‚¡ç¥¨ä»£ç ï¼ˆå»æ‰.SS/.SZåç¼€ï¼‰
        discovered = []
        for f in ts_dir.glob('*.csv'):
            # æ–‡ä»¶åæ ¼å¼: 600000.SS.csv æˆ– 000001.SZ.csv
            stock_code = f.stem.replace('.SS', '').replace('.SZ', '')
            discovered.append(stock_code)

        print(f"ğŸ” è‡ªåŠ¨å‘ç° {len(discovered)} ä¸ªè‚¡ç¥¨ï¼Œä½¿ç”¨å‰10ä¸ªè¿›è¡Œæµ‹è¯•")
        return discovered[:10]
    
    def _find_kline_image(self, stock, target_date):
        """æŸ¥æ‰¾Kçº¿å›¾æ–‡ä»¶"""
        if self.is_finmultime:
            # Finmultimeç»“æ„: image/HS300_image/HS300_image/ (åªæœ‰HS300æœ‰å›¾åƒ)
            if self.market == 'HS300':
                img_dir = self.data_dir / 'image' / 'HS300_image' / 'HS300_image'
            else:
                # SP500åœ¨Finmultimeä¸­æ²¡æœ‰å›¾åƒæ•°æ®
                return None
        else:
            # FinMultiTimeæ ‡å‡†ç»“æ„
            img_dir = self.data_dir / 'image'

        if not img_dir.exists():
            return None

        target_date = pd.to_datetime(target_date)
        year = target_date.year
        half = 'H1' if target_date.month <= 6 else 'H2'

        # å°è¯•å¤šç§å‘½åæ ¼å¼
        for pattern in [f'{stock}_{year}{half}.png',
                       f'{stock}_{year}_{half}.png',
                       f'{stock}_{year}-{half}.png']:
            path = img_dir / pattern
            if path.exists():
                return path
        return None
    
    def _extract_image_features_cnn(self, stock, dates):
        """ä½¿ç”¨æ··åˆCNNæå–Kçº¿ç‰¹å¾"""
        if not self.use_cnn_features:
            return np.full((len(dates), 1), 0.5)
        
        features = []
        current_image_path = None
        current_features = None
        
        for date in dates:
            img_path = self._find_kline_image(stock, date)
            
            # å¦‚æœå›¾åƒæ”¹å˜ï¼Œé‡æ–°æå–ç‰¹å¾
            if img_path and img_path != current_image_path:
                try:
                    img = Image.open(img_path).convert('L')
                    img_tensor = self.image_transform(img).unsqueeze(0)
                    
                    with torch.no_grad():
                        feat = self.image_encoder(img_tensor)
                    current_features = feat.squeeze(0).numpy()
                    current_image_path = img_path
                except:
                    current_features = np.zeros(self.image_encoder.output_projection[-1].out_features)
            
            if current_features is not None:
                features.append(current_features)
            else:
                features.append(np.zeros(64))  # é»˜è®¤ç‰¹å¾ç»´åº¦
        
        return np.array(features)
    
    def _get_stock_suffix(self, stock):
        """è·å–è‚¡ç¥¨çš„å¸‚åœºåç¼€ (.SS æˆ– .SZ)"""
        if self.is_finmultime and self.market == 'HS300':
            # HS300è‚¡ç¥¨ï¼š6å¼€å¤´æ˜¯ä¸Šæµ·(.SS)ï¼Œ0/3å¼€å¤´æ˜¯æ·±åœ³(.SZ)
            if stock.startswith('6'):
                return '.SS'
            elif stock.startswith(('0', '3')):
                return '.SZ'
        return ''

    def _load_and_align_data(self):
        """åŠ è½½å¹¶å¯¹é½æ‰€æœ‰æ¨¡æ€"""
        failed_stocks = []

        for stock in self.stocks:
            try:
                print(f"Processing {stock}...")

                # è·å–è‚¡ç¥¨åç¼€
                suffix = self._get_stock_suffix(stock)
                stock_with_suffix = f"{stock}{suffix}"

                # æ„å»ºè·¯å¾„ï¼ˆé€‚é…Finmultimeå’ŒFinMultiTimeï¼‰
                if self.is_finmultime:
                    # Finmultimeç»“æ„
                    if self.market == 'SP500':
                        # æ³¨æ„ï¼šå¤–å±‚ç›®å½•æ˜¯ S%26P500_time_series (URLç¼–ç )ï¼Œå†…å±‚æ˜¯ S&P500_time_series
                        ts_path = self.data_dir / 'time_series' / 'S%26P500_time_series' / 'S&P500_time_series' / f'{stock_with_suffix}.csv'
                        text_path = self.data_dir / 'text' / 'sp500_news' / 'sp500_news' / f'{stock_with_suffix}.jsonl'
                        table_path = self.data_dir / 'table' / 'SP500_tabular' / f'{stock_with_suffix}.json'
                    else:  # HS300
                        ts_path = self.data_dir / 'time_series' / 'HS300_time_series' / 'HS300_time_series' / f'{stock_with_suffix}.csv'
                        text_path = self.data_dir / 'text' / 'hs300news_summary' / 'hs300news_summary' / f'{stock_with_suffix}.jsonl'
                        table_path = self.data_dir / 'table' / 'hs300_tabular' / 'hs300_tabular' / f'{stock_with_suffix}' / 'income.jsonl'
                else:
                    # FinMultiTimeæ ‡å‡†ç»“æ„
                    ts_path = self.data_dir / 'time_series' / f'{stock}.csv'
                    text_path = self.data_dir / 'text' / f'{stock}.jsonl'
                    table_path = self.data_dir / 'table' / f'{stock}.json'

                # åŠ è½½æ—¶åºæ•°æ®
                ts_result = load_time_series(ts_path, self.start_date, self.end_date)

                if ts_result is None:
                    continue

                ts_df, dates = ts_result

                if len(dates) < self.seq_length + self.pred_horizon:
                    continue

                ts_features = ts_df.drop('Date', axis=1).values

                # åŠ è½½å…¶ä»–æ¨¡æ€
                news = load_news_sentiment(text_path, dates)

                # ä½¿ç”¨CNNæå–Kçº¿ç‰¹å¾
                image_features = self._extract_image_features_cnn(stock, dates)

                fundamentals = load_fundamentals(table_path, dates)

                # å½’ä¸€åŒ–
                ts_norm = self.preprocessor.fit_transform_time_series(
                    ts_features, f'{stock}_ts'
                )
                fund_norm = self.preprocessor.fit_transform_fundamentals(
                    fundamentals, f'{stock}_fund'
                )

                # æ‹¼æ¥: 6 + 1 + 64 + 6 = 77ç»´
                multimodal_features = np.concatenate([
                    ts_norm,           # 6ç»´
                    news,              # 1ç»´
                    image_features,    # 64ç»´
                    fund_norm          # 6ç»´
                ], axis=1)

                # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
                for i in range(len(multimodal_features) - self.seq_length - self.pred_horizon + 1):
                    x = multimodal_features[i:i + self.seq_length]
                    y = ts_norm[i + self.seq_length:i + self.seq_length + self.pred_horizon, 3]

                    self.samples.append({
                        'stock': stock,
                        'x': torch.FloatTensor(x),
                        'y': torch.FloatTensor(y),
                        'date': dates[i + self.seq_length - 1],
                        'scaler_key': f'{stock}_ts'
                    })

                print(f"  Created {len([s for s in self.samples if s['stock']==stock])} samples")

            except Exception as e:
                print(f"  âš ï¸  Failed to process {stock}: {str(e)}")
                failed_stocks.append(stock)
                continue

        # æ‰“å°æ€»ç»“
        if failed_stocks:
            print(f"\nâš ï¸  Failed to load {len(failed_stocks)} stocks: {failed_stocks[:10]}{'...' if len(failed_stocks) > 10 else ''}")
        print(f"âœ… Successfully loaded {len(self.stocks) - len(failed_stocks)} stocks with {len(self.samples)} total samples\n")

    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        return self.samples[idx]['x'], self.samples[idx]['y']


# ============================================================================
# data/__init__.py
# ============================================================================
# from .dataset import FinMultiTimeDataset
# from .dataloader import create_dataloaders
# __all__ = ['FinMultiTimeDataset', 'create_dataloaders']
