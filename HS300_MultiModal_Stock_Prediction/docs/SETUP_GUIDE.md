# ğŸ› ï¸ HS300 å¤šæ¨¡æ€è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ - å®‰è£…é…ç½®æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå®‰è£…](#å¿«é€Ÿå®‰è£…)
2. [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
3. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
4. [é…ç½®é€‰é¡¹](#é…ç½®é€‰é¡¹)
5. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## å¿«é€Ÿå®‰è£…

### âš¡ ä¸€é”®å®‰è£…è„šæœ¬

```bash
# 1. åˆ›å»ºç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
conda create -n hs300 python=3.9 -y
conda activate hs300
pip install -r requirements.txt

# 2. éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"
python -c "import numpy; print(f'NumPyç‰ˆæœ¬: {numpy.__version__}')"

# 3. å¿«é€Ÿæµ‹è¯•
python extract_stocks.py
python train.py --mode train --epochs 1 --stocks 1
```

### ğŸ“‹ ç³»ç»Ÿè¦æ±‚

| ç»„ä»¶ | æœ€ä½è¦æ±‚ | æ¨èé…ç½® |
|------|----------|----------|
| **æ“ä½œç³»ç»Ÿ** | Windows 10, Linux, macOS | Ubuntu 20.04+, Windows 11 |
| **Python** | 3.8+ | 3.9-3.11 |
| **å†…å­˜** | 8GB | 32GB+ |
| **GPU** | å¯é€‰ | NVIDIA RTX 3080+ |
| **å­˜å‚¨** | 20GB | 100GB+ SSD |
| **CUDA** | å¯é€‰ | 11.8+ |

---

## è¯¦ç»†é…ç½®

### 1. ç¯å¢ƒå‡†å¤‡

#### æ–¹å¼ä¸€ï¼šCondaç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
# åˆ›å»ºæ–°ç¯å¢ƒ
conda create -n hs300 python=3.9 -y
conda activate hs300

# å®‰è£…PyTorchï¼ˆæ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CPUç‰ˆæœ¬
conda install pytorch torchvision cpuonly -c pytorch

# GPUç‰ˆæœ¬ï¼ˆCUDA 11.8ï¼‰
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

#### æ–¹å¼äºŒï¼šè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv hs300_env

# æ¿€æ´»ç¯å¢ƒ
# Windows
hs300_env\Scripts\activate
# Linux/Mac
source hs300_env/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. ä¾èµ–å®‰è£…

#### åŸºç¡€å®‰è£…ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰

```bash
pip install -r requirements.txt
```

#### é«˜çº§åŠŸèƒ½å®‰è£…ï¼ˆå¯é€‰ï¼‰

```bash
# BERTæ–‡æœ¬ç¼–ç å™¨
pip install transformers tokenizers

# é¢„è®­ç»ƒå›¾åƒæ¨¡å‹
pip install timm

# å¼€å‘å·¥å…·
pip install tensorboard wandb pytest black flake8
```

### 3. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥æ ¸å¿ƒä¾èµ–
python -c "
import torch
import numpy as np
import pandas as pd
import sklearn
print('âœ… æ ¸å¿ƒä¾èµ–å®‰è£…æˆåŠŸ')
print(f'PyTorch: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
"

# æ£€æŸ¥é«˜çº§åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
python -c "
try:
    import transformers
    print('âœ… Transformerså¯ç”¨')
except ImportError:
    print('âš ï¸ Transformersæœªå®‰è£…ï¼ˆé«˜çº§æ–‡æœ¬åŠŸèƒ½ä¸å¯ç”¨ï¼‰')

try:
    import timm
    print('âœ… TIMMå¯ç”¨')
except ImportError:
    print('âš ï¸ TIMMæœªå®‰è£…ï¼ˆé«˜çº§å›¾åƒåŠŸèƒ½ä¸å¯ç”¨ï¼‰')
"
```

```python
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

---

## æ•°æ®å‡†å¤‡

### æ•°æ®ç»“æ„

é¡¹ç›®éœ€è¦ `Finmultime` æ•°æ®ç›®å½•ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```
Finmultime/
â”œâ”€â”€ time_series/
â”‚   â””â”€â”€ HS300_time_series/
â”‚       â””â”€â”€ HS300_time_series/
â”‚           â”œâ”€â”€ 600000.SS.csv
â”‚           â”œâ”€â”€ 000001.SZ.csv
â”‚           â””â”€â”€ ...
â”œâ”€â”€ image/
â”‚   â””â”€â”€ HS300_image/
â”‚       â””â”€â”€ HS300_image/
â”‚           â”œâ”€â”€ 600000.SS_2019-01-02.png
â”‚           â””â”€â”€ ...
â”œâ”€â”€ text/
â”‚   â””â”€â”€ hs300news_summary/
â”‚       â””â”€â”€ hs300news_summary/
â”‚           â”œâ”€â”€ 600000.SS.jsonl
â”‚           â””â”€â”€ ...
â””â”€â”€ table/
    â””â”€â”€ hs300_tabular/
        â””â”€â”€ hs300_tabular/
            â”œâ”€â”€ 600000.SS/
            â”‚   â””â”€â”€ income.jsonl
            â””â”€â”€ ...
```

### æ•°æ®æ”¾ç½®

**é€‰é¡¹1**: å°† `Finmultime` ç›®å½•æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ä¸Šä¸€çº§

```
parent_directory/
â”œâ”€â”€ Finmultime/          # æ•°æ®ç›®å½•
â””â”€â”€ HS300_MultiModal_Stock_Prediction/  # é¡¹ç›®ç›®å½•
```

**é€‰é¡¹2**: ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„

ç¼–è¾‘ `configs/config.py`:

```python
@dataclass
class DataConfig:
    data_dir: str = 'E:/path/to/your/Finmultime'  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
    ...
```

### æå–å®Œæ•´å››æ¨¡æ€è‚¡ç¥¨

è¿è¡Œæå–è„šæœ¬ï¼š

```bash
python extract_stocks.py
```

è¿™å°†ç”Ÿæˆ `hs300_complete_stocks.json`ï¼ŒåŒ…å«790ä¸ªå®Œæ•´å››æ¨¡æ€è‚¡ç¥¨çš„ä¿¡æ¯ã€‚

---

## é…ç½®è¯´æ˜

### configs/config.py

#### DataConfig - æ•°æ®é…ç½®

```python
@dataclass
class DataConfig:
    # æ•°æ®ç›®å½•
    data_dir: str = '../Finmultime'
    
    # å¸‚åœºé€‰æ‹©
    market: str = 'HS300'  # 'HS300' æˆ– 'SP500'
    
    # è‚¡ç¥¨åˆ—è¡¨
    stocks: List[str] = None  # None=è‡ªåŠ¨åŠ è½½æ‰€æœ‰790ä¸ªè‚¡ç¥¨
    # æˆ–æŒ‡å®šç‰¹å®šè‚¡ç¥¨: ['600000', '600004', '600006']
    
    # æ—¥æœŸèŒƒå›´
    start_date: str = '2019-01-01'
    end_date: str = '2025-12-31'
    
    # åºåˆ—å‚æ•°
    seq_length: int = 60        # è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå¤©ï¼‰
    pred_horizon: int = 24      # é¢„æµ‹æ­¥é•¿ï¼ˆå¤©ï¼‰
    
    # ç‰¹å¾é…ç½®
    use_cnn_features: bool = True   # æ˜¯å¦ä½¿ç”¨CNNæå–å›¾åƒç‰¹å¾
    cnn_feature_dim: int = 64       # CNNç‰¹å¾ç»´åº¦
```

#### ModelConfig - æ¨¡å‹é…ç½®

```python
@dataclass
class ModelConfig:
    # è¾“å…¥ç»´åº¦: 6(ts) + 1(text) + 64(image) + 6(table) = 77
    input_size: int = 77
    
    # éšè—å±‚ç»´åº¦
    hidden_size: int = 128
    
    # TKANå±‚æ•°
    num_layers: int = 2
    
    # Dropoutç‡
    dropout: float = 0.2
```

#### TrainConfig - è®­ç»ƒé…ç½®

```python
@dataclass
class TrainConfig:
    # æ‰¹æ¬¡å¤§å°
    batch_size: int = 32
    
    # è®­ç»ƒè½®æ•°
    epochs: int = 100
    
    # å­¦ä¹ ç‡
    learning_rate: float = 0.001
    
    # æƒé‡è¡°å‡
    weight_decay: float = 1e-5
    
    # æ—©åœè€å¿ƒå€¼
    patience: int = 10
    
    # éšæœºç§å­
    seed: int = 42
    
    # ä¿å­˜ç›®å½•
    checkpoint_dir: str = './checkpoints'
    log_dir: str = './logs'
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å¿«é€Ÿè®­ç»ƒæµ‹è¯• (5åˆ†é’Ÿ)

ä½¿ç”¨3ä¸ªè‚¡ç¥¨ï¼Œè®­ç»ƒ3ä¸ªepochï¼š

```bash
python quick_train.py --epochs 3 --test_stocks 3
```

### 2. ä¸­ç­‰è§„æ¨¡è®­ç»ƒ (30åˆ†é’Ÿ)

ä½¿ç”¨10ä¸ªè‚¡ç¥¨ï¼Œè®­ç»ƒ10ä¸ªepochï¼š

```bash
python quick_train.py --epochs 10 --test_stocks 10 --batch_size 32
```

### 3. å®Œæ•´è®­ç»ƒ (æ•°å°æ—¶)

ä½¿ç”¨æ‰€æœ‰790ä¸ªè‚¡ç¥¨ï¼Œè®­ç»ƒ100ä¸ªepochï¼š

```bash
python train.py --epochs 100 --batch_size 64
```

### 4. è¯„ä¼°æ¨¡å‹

```bash
python evaluate.py --checkpoint checkpoints/best_model.pth
```

### 5. ç”Ÿæˆå¯è§†åŒ–

```bash
python visualize.py --mode all
```

---

## é«˜çº§é…ç½®

### ä½¿ç”¨GPUåŠ é€Ÿ

ç¡®ä¿å®‰è£…äº†CUDAç‰ˆæœ¬çš„PyTorchï¼š

```bash
# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
python -c "import torch; print(torch.cuda.is_available())"

# å¦‚æœä¸å¯ç”¨ï¼Œå®‰è£…CUDAç‰ˆæœ¬
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### è‡ªå®šä¹‰è‚¡ç¥¨åˆ—è¡¨

#### æ–¹æ³•1: ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/config.py`:

```python
@dataclass
class DataConfig:
    stocks: List[str] = ['600000', '600004', '600006', '600008', '600009']
```

#### æ–¹æ³•2: æŒ‰è¡Œä¸šç­›é€‰

```python
import json

# åŠ è½½è‚¡ç¥¨ä¿¡æ¯
with open('hs300_complete_stocks.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# ç­›é€‰åŒ»è¯ç”Ÿç‰©è¡Œä¸š
pharma_stocks = [
    stock['è¯åˆ¸ä»£ç '].replace('.SS', '').replace('.SZ', '')
    for stock in data['stock_info']
    if stock['ç”³ä¸‡ä¸€çº§è¡Œä¸š'] == 'åŒ»è¯ç”Ÿç‰©'
]

print(f"åŒ»è¯ç”Ÿç‰©è¡Œä¸šè‚¡ç¥¨: {len(pharma_stocks)}ä¸ª")
print(pharma_stocks[:10])
```

### è°ƒæ•´è¶…å‚æ•°

#### å­¦ä¹ ç‡è°ƒåº¦

åœ¨ `train.py` ä¸­æ·»åŠ ï¼š

```python
from torch.optim.lr_scheduler import ReduceLROnPlateau

scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=5
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(epochs):
    # ... è®­ç»ƒä»£ç  ...
    scheduler.step(val_loss)
```

#### æ—©åœ

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=10)
for epoch in range(epochs):
    # ... è®­ç»ƒä»£ç  ...
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print("Early stopping triggered")
        break
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: æ•°æ®é›†ä¸ºç©º

**ç—‡çŠ¶**: `æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œæ€»æ ·æœ¬æ•°: 0`

**åŸå› **:
- æ•°æ®æ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®
- æ—¥æœŸèŒƒå›´ä¸åŒ¹é…
- æ–‡ä»¶å‘½åæ ¼å¼ä¸å¯¹

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ£€æŸ¥æ•°æ®ç›®å½•
ls ../Finmultime/time_series/HS300_time_series/HS300_time_series/ | head

# 2. æ£€æŸ¥æ–‡ä»¶æ ¼å¼
python -c "
import pandas as pd
df = pd.read_csv('../Finmultime/time_series/HS300_time_series/HS300_time_series/600000.SS.csv')
print('Columns:', df.columns.tolist())
print('Date range:', df['Date'].min(), 'to', df['Date'].max())
"

# 3. éªŒè¯è‚¡ç¥¨ä»£ç 
python -c "
from pathlib import Path
ts_dir = Path('../Finmultime/time_series/HS300_time_series/HS300_time_series')
files = list(ts_dir.glob('*.csv'))
print(f'Found {len(files)} CSV files')
print('First 5:', [f.stem for f in files[:5]])
"
```

### é—®é¢˜2: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory` æˆ–ç³»ç»Ÿå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:

1. å‡å°‘batch_size:
```bash
python quick_train.py --batch_size 8
```

2. å‡å°‘è‚¡ç¥¨æ•°é‡:
```bash
python quick_train.py --test_stocks 5
```

3. å‡å°‘åºåˆ—é•¿åº¦ï¼ˆä¿®æ”¹é…ç½®ï¼‰:
```python
seq_length: int = 30  # ä»60å‡å°‘åˆ°30
```

### é—®é¢˜3: è®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**:

1. ä½¿ç”¨GPU:
```bash
# æ£€æŸ¥GPUæ˜¯å¦è¢«ä½¿ç”¨
python -c "import torch; print(torch.cuda.is_available())"
```

2. å¢åŠ batch_size (å¦‚æœå†…å­˜å…è®¸):
```bash
python train.py --batch_size 128
```

3. ä½¿ç”¨æ•°æ®å¹¶è¡Œ:
```python
if torch.cuda.device_count() > 1:
    model = torch.nn.DataParallel(model)
```

### é—®é¢˜4: æ¨¡å‹ä¸æ”¶æ•›

**ç—‡çŠ¶**: æŸå¤±ä¸ä¸‹é™æˆ–æ³¢åŠ¨å¾ˆå¤§

**è§£å†³æ–¹æ¡ˆ**:

1. é™ä½å­¦ä¹ ç‡:
```python
learning_rate: float = 0.0001  # ä»0.001é™ä½
```

2. å¢åŠ è®­ç»ƒè½®æ•°:
```bash
python train.py --epochs 200
```

3. æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–:
```python
# åœ¨dataset.pyä¸­ç¡®ä¿æ•°æ®å·²å½’ä¸€åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data = scaler.fit_transform(data)
```

### é—®é¢˜5: æ–‡ä»¶ç¼–ç é”™è¯¯

**ç—‡çŠ¶**: `UnicodeDecodeError` æˆ–ä¸­æ–‡ä¹±ç 

**è§£å†³æ–¹æ¡ˆ**:

```python
# è¯»å–CSVæ—¶æŒ‡å®šç¼–ç 
df = pd.read_csv(file_path, encoding='utf-8')
# æˆ–
df = pd.read_csv(file_path, encoding='gbk')

# è¯»å–JSONæ—¶æŒ‡å®šç¼–ç 
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®
train_loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,  # å¢åŠ workeræ•°é‡
    pin_memory=True  # å¦‚æœä½¿ç”¨GPU
)
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(epochs):
    for X, y in train_loader:
        optimizer.zero_grad()
        
        with autocast():
            output = model(X)
            loss = criterion(output, y)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
```

### 3. æ¢¯åº¦ç´¯ç§¯

```python
accumulation_steps = 4

for i, (X, y) in enumerate(train_loader):
    output = model(X)
    loss = criterion(output, y) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## è”ç³»æ”¯æŒ

å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ [README.md](../README.md)
2. æŸ¥çœ‹ [APIæ–‡æ¡£](API_REFERENCE.md)
3. æäº¤Issue

---

**æœ€åæ›´æ–°**: 2025-10-07

